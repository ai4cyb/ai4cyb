{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cbc3ef",
   "metadata": {},
   "source": [
    "Project 2: Model Engineering\n",
    "===\n",
    "\n",
    "___\n",
    "\n",
    "Submitted by:\n",
    "\n",
    "* <u>*Arthur Humblot*</u>\n",
    "* <u>*Bekhzod Anvarov*</u>\n",
    "* <u>*Ghita El Belghiti*</u>\n",
    "\n",
    "\n",
    "University: **Politechnico di Torino**\n",
    "\n",
    "Academic Year: **2025 - 2026**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:23:25.686069Z",
     "start_time": "2025-12-14T00:23:25.683621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#imports here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import time"
   ],
   "id": "635960c64bccda3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "d28f9fa6",
   "metadata": {},
   "source": [
    "## 1. Task 1: Frequency-based baseline"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In Machine Learning problems, it is always good practice to compare against baseline solutions. Typically, one baseline involves a simple approach that helps determine whether simple choices and assumptions can already address the problem - before progressing to potentially more complex architectures like RNNs or GNNs.\n",
    "\n",
    "In this context, a suitable baseline is a **frequency-based** approach.\n",
    "\n",
    "Specifically:"
   ],
   "id": "d506221655f96d55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.540436Z",
     "start_time": "2025-12-13T21:45:48.255014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read dataset\n",
    "df_train = pd.read_json(\"../data/train.json\")\n",
    "df_test = pd.read_json(\"../data/test.json\")\n",
    "\n",
    "# instruction check\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ],
   "id": "957f594eb354418b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   api_call_sequence  is_malware\n",
      "0  [LdrGetDllHandle, LdrGetProcedureAddress, LdrL...           1\n",
      "1  [NtAllocateVirtualMemory, LdrLoadDll, LdrGetPr...           1\n",
      "2  [FindResourceExW, LoadResource, FindResourceEx...           1\n",
      "3  [FindResourceExW, LoadResource, FindResourceEx...           1\n",
      "4  [LdrGetProcedureAddress, SetErrorMode, LdrLoad...           1\n",
      "                                   api_call_sequence  is_malware\n",
      "0  [NtQueryValueKey, NtClose, NtOpenKey, NtQueryV...           1\n",
      "1  [LdrGetProcedureAddress, NtClose, NtOpenKey, N...           1\n",
      "2  [NtOpenKey, NtQueryValueKey, NtClose, NtOpenKe...           1\n",
      "3  [NtAllocateVirtualMemory, LdrLoadDll, LdrGetPr...           1\n",
      "4  [NtOpenKey, NtQueryValueKey, NtClose, LdrGetPr...           1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Extract the vocabulary from your input dataset - that is, the **set of all the API calls** appearing in it",
   "id": "547b02adba1dfb4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.555629Z",
     "start_time": "2025-12-13T21:45:48.551886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract sequences(api) and labels\n",
    "train_seqs = df_train['api_call_sequence'].tolist()\n",
    "test_seqs = df_test['api_call_sequence'].tolist()\n",
    "\n",
    "train_labels = df_train['is_malware'].tolist()\n",
    "test_labels = df_test['is_malware'].tolist()\n",
    "\n",
    "# instruction check\n",
    "print(train_seqs[0][:5])\n",
    "print(f\"Type of sequence: {type(train_seqs[0]).__name__}\")"
   ],
   "id": "2a8afa56c4cff397",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LdrGetDllHandle', 'LdrGetProcedureAddress', 'LdrLoadDll', 'LdrGetProcedureAddress', 'LdrGetDllHandle']\n",
      "Type of sequence: list\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* **Q:** How many unique API calls does the training set contain?",
   "id": "46516f71c75af16a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.681427Z",
     "start_time": "2025-12-13T21:45:48.604907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create train vocabulary unique api\n",
    "train_vocab = set()\n",
    "\n",
    "for train_seq in train_seqs:\n",
    "    for api_call in train_seq:\n",
    "        train_vocab.add(api_call)\n",
    "\n",
    "print(f\"Number of unique API calls the training set contain: {len(train_vocab)}\")"
   ],
   "id": "e99b9d739ee25be3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique API calls the training set contain: 258\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And how many the test set?",
   "id": "90299d725737735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.731481Z",
     "start_time": "2025-12-13T21:45:48.690789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test vocal unique api\n",
    "test_vocab = set()\n",
    "\n",
    "for test_seq in test_seqs:\n",
    "    for api_call in test_seq:\n",
    "        test_vocab.add(api_call)\n",
    "\n",
    "print(f\"Number of unique API calls the test set contain: {len(test_vocab)}\")"
   ],
   "id": "c04e11cf73bf9f08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique API calls the test set contain: 232\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* **Q:** Are there any API calls that appear only in the test set (but not in the training set)? If yes, how many? And which one are they?",
   "id": "87294fd8c98f5826"
  },
  {
   "cell_type": "code",
   "id": "108f9e91",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.767253Z",
     "start_time": "2025-12-13T21:45:48.764774Z"
    }
   },
   "source": [
    "# features, which appear only in test set, and not in train set\n",
    "only_in_test = test_vocab - train_vocab\n",
    "print(f\"Number of unique API calls only the test set contain(but not in the training set): {len(only_in_test)}\")\n",
    "print(f\"Unique API calls only the test set contain:\\n{only_in_test}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique API calls only the test set contain(but not in the training set): 3\n",
      "Unique API calls only the test set contain:\n",
      "{'WSASocketA', 'ControlService', 'NtDeleteKey'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.818598Z",
     "start_time": "2025-12-13T21:45:48.815921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sorted vocabulary\n",
    "train_vocab_sorted = sorted([i for i in train_vocab])\n",
    "test_vocab_sorted = sorted([i for i in test_vocab])\n",
    "\n",
    "# instruction check\n",
    "print(train_vocab_sorted[:5])\n",
    "print(test_vocab_sorted[:5])"
   ],
   "id": "882d16481fb09898",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CertOpenStore', 'CertOpenSystemStoreW', 'CoCreateInstance', 'CoCreateInstanceEx', 'CoGetClassObject']\n",
      "['CoCreateInstance', 'CoCreateInstanceEx', 'CoGetClassObject', 'CoInitializeEx', 'CoInitializeSecurity']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* **Q:** Can you use the test vocabulary to build the new test dataframe? If not, how do you handle API calls in the test set that do not exist in the training vocabulary?",
   "id": "3a5b1a57dc850a7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:48.869325Z",
     "start_time": "2025-12-13T21:45:48.867146Z"
    }
   },
   "cell_type": "code",
   "source": "feature_names = train_vocab_sorted + ['<UNK>']",
   "id": "d8b562e4ddeaa11d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We add **< UNK >** - for features unknown for train set and appears on test set only",
   "id": "ab5bb8973a305674"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Use this vocabulary as the **feature set**: for each row in the input dataset, count the **number of times** (frequency) each vocabulary term occurs",
   "id": "5c46374e99d9fdf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:49.248382Z",
     "start_time": "2025-12-13T21:45:48.914719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# map api with their positions\n",
    "api_to_idx = dict()\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    api_to_idx[feature_names[i]] = i\n",
    "\n",
    "# creating features for train\n",
    "X_train = list()\n",
    "\n",
    "for seq in train_seqs:\n",
    "    freq = [0 for _ in range(len(feature_names))]   # frequency vector for train features\n",
    "    for api_call in seq:\n",
    "        if api_call in api_to_idx:\n",
    "            freq[api_to_idx[api_call]] += 1\n",
    "    X_train.append(freq)\n",
    "\n",
    "# creating features for train\n",
    "X_test = list()\n",
    "\n",
    "for seq in test_seqs:\n",
    "    freq = [0 for _ in range(len(feature_names))]   # frequency vector for test features\n",
    "    for api_call in seq:\n",
    "        if api_call in api_to_idx:\n",
    "            freq[api_to_idx[api_call]] += 1     # UNK features, which are only on test\n",
    "        else:\n",
    "            freq[-1] += 1\n",
    "    X_test.append(freq)"
   ],
   "id": "f68bbf7b72d1deff",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* **Q:** One issue of this frequency-based approach is that it creates sparse vectors (i.e., vectors with many zeros per row):\n",
    "    * how many non-zero elements per row do you have on average in the training set?\n",
    "    * How many in the test set ?\n",
    "    * What is the ratio with respect to the number of elements per row?"
   ],
   "id": "4644eae288a8bf2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:49.371698Z",
     "start_time": "2025-12-13T21:45:49.256821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sparsity for the train set (non-zero per row)\n",
    "nnz_train_per_row = list()  # num of non zeros\n",
    "\n",
    "for freq in X_train:\n",
    "    nnz_train_per_row.append(sum([1 if i > 0 else 0 for i in freq]))\n",
    "\n",
    "avg_non_zero_train = sum(nnz_train_per_row) / len(X_train)\n",
    "print(f\"Average non-zero elements per row in training set: {avg_non_zero_train:.2f}\")\n",
    "ratio_train = avg_non_zero_train / len(feature_names)\n",
    "print(f\"Ratio with respect to the number of elements per row in training set: {ratio_train:.2f}\\n\")\n",
    "\n",
    "\n",
    "# sparsity for the test set (non-zero per row)\n",
    "nnz_test_per_row = list()  # num of non zeros\n",
    "\n",
    "for freq in X_test:\n",
    "    nnz_test_per_row.append(sum([1 if i > 0 else 0 for i in freq]))\n",
    "\n",
    "avg_non_zero_test = sum(nnz_test_per_row) / len(X_test)\n",
    "print(f\"Average non-zero elements per row in test set: {avg_non_zero_test:.2f}\")\n",
    "ratio_test = avg_non_zero_test / len(feature_names)\n",
    "print(f\"Ratio with respect to the number of elements per row in test set: {ratio_test:.2f}\")"
   ],
   "id": "8f3651049db881f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average non-zero elements per row in training set: 21.95\n",
      "Ratio with respect to the number of elements per row in training set: 0.08\n",
      "\n",
      "Average non-zero elements per row in test set: 24.28\n",
      "Ratio with respect to the number of elements per row in test set: 0.09\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "187819eb",
   "metadata": {},
   "source": [
    "## 2. Task 2: Feed Forward Neural Network (FFNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c9aa1",
   "metadata": {},
   "source": [
    "## 3. Task 3: Recurrent Neural Network (RNN)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Preprocessing of Data",
   "id": "42999cda7edd6a77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:15:48.902737Z",
     "start_time": "2025-12-14T00:15:48.897532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# splitting to train and validation sets\n",
    "train_seqs, val_seqs, train_labels, val_labels  = train_test_split(train_seqs, train_labels, test_size=0.2, random_state=42)"
   ],
   "id": "76543685b68fddaf",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:16:46.917924Z",
     "start_time": "2025-12-14T00:16:46.791505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# adding pad entry to api_to_idx\n",
    "api_to_idx['<PAD>'] = len(api_to_idx)\n",
    "\n",
    "# unknown and padding indexes\n",
    "unk_index = api_to_idx['<UNK>']\n",
    "pad_index = api_to_idx['<PAD>']\n",
    "\n",
    "# convert each api call string to numeric value - index on train set\n",
    "train_ids = list()\n",
    "for seq in train_seqs:\n",
    "    seq_ids = list()\n",
    "    for api_call in seq:\n",
    "        if api_call in api_to_idx:\n",
    "            seq_ids.append(api_to_idx[api_call])\n",
    "        else:\n",
    "            seq_ids.append(api_to_idx['<UNK>'])\n",
    "    train_ids.append(seq_ids)\n",
    "\n",
    "# convert each api call string to numeric value - index on val set\n",
    "val_ids = list()\n",
    "for seq in val_seqs:\n",
    "    seq_ids = list()\n",
    "    for api_call in seq:\n",
    "        if api_call in api_to_idx:\n",
    "            seq_ids.append(api_to_idx[api_call])\n",
    "        else:\n",
    "            seq_ids.append(api_to_idx['<UNK>'])\n",
    "    val_ids.append(seq_ids)\n",
    "\n",
    "# convert each api call string to numeric value - index on test set\n",
    "test_ids = list()\n",
    "for seq in test_seqs:\n",
    "    seq_ids = list()\n",
    "    for api_call in seq:\n",
    "        if api_call in api_to_idx:\n",
    "            seq_ids.append(api_to_idx[api_call])\n",
    "        else:\n",
    "            seq_ids.append(api_to_idx['<UNK>'])\n",
    "    test_ids.append(seq_ids)"
   ],
   "id": "a149e658e51f9bac",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:49.592598Z",
     "start_time": "2025-12-13T21:45:49.589460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom Dataset\n",
    "class APICallDataset(Dataset):\n",
    "    def __init__(self, ids: list[list[int]], labels: list[int]):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sequence = torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sequence, label"
   ],
   "id": "67ae51ee3d54f0d8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:45:49.640763Z",
     "start_time": "2025-12-13T21:45:49.638200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# collate function to add padding per each batch\n",
    "def collate_fn(batch):\n",
    "    ids, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in ids])\n",
    "\n",
    "    sorted_indices = lengths.argsort(descending=True)\n",
    "    ids = [ids[i] for i in sorted_indices]\n",
    "    labels = torch.stack([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    padded_ids = pad_sequence(ids, batch_first=True, padding_value=pad_index)\n",
    "\n",
    "    return padded_ids, lengths, labels"
   ],
   "id": "c0d8a4825082cca8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:17:48.979746Z",
     "start_time": "2025-12-14T00:17:48.973985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = APICallDataset(train_ids, train_labels)\n",
    "val_dataset = APICallDataset(val_ids, val_labels)\n",
    "test_dataset = APICallDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn, shuffle=False)"
   ],
   "id": "75e80a9db1db60e2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Recurrent Neural Network Models",
   "id": "339c8387539da0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Monodirectional RNN model",
   "id": "a5b5206ce1633810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T23:41:35.785046Z",
     "start_time": "2025-12-13T23:41:35.781402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonoRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 num_layers: int,\n",
    "                 pad_idx: int):\n",
    "        super(MonoRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, h = self.rnn(packed_embedded)\n",
    "        return self.linear(h[-1])"
   ],
   "id": "55d68b08549a4fd4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bidirectional RNN model",
   "id": "10fd03b62aace620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:24:21.191885Z",
     "start_time": "2025-12-14T00:24:21.188315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 num_layers: int,\n",
    "                 pad_idx: int):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_out, h = self.rnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        hidden_cat = torch.cat((h[-2], h[-1]), dim=1)\n",
    "        return self.linear(hidden_cat)"
   ],
   "id": "f9b7ffaa83de64cc",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Monodirectional LSTM RNN model",
   "id": "7ef8ec07bfb2be40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T23:41:43.872820Z",
     "start_time": "2025-12-13T23:41:43.868945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonoLSTMRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 num_layers: int,\n",
    "                 pad_idx: int):\n",
    "        super(MonoLSTMRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, (h, cell) = self.lstm(packed_embedded)\n",
    "        return self.linear(h[-1])"
   ],
   "id": "c476f7762d35342",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bidirectional LSTM RNN model",
   "id": "9dc557571191716b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:24:38.756778Z",
     "start_time": "2025-12-14T00:24:38.753210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiLSTMRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 num_layers: int,\n",
    "                 pad_idx: int):\n",
    "        super(BiLSTMRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        packed_out, (h, cell) = self.lstm(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        hidden_cat = torch.cat((h[-2], h[-1]), dim=1)\n",
    "\n",
    "        return self.linear(hidden_cat)"
   ],
   "id": "98a8c802fa4b4c9e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "03658c14",
   "metadata": {},
   "source": "## 4. Task 4: Graph Neural Network (GNN)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
